{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1: Configuración e Instalaciones\n",
        "Esta sección se encarga de preparar el entorno de Colab, montando Google Drive para la gestión de archivos y asegurándose de que todas las librerías necesarias estén instaladas."
      ],
      "metadata": {
        "id": "ELGC7sHyMTpD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECCIÓN 1: CONFIGURACIÓN E INSTALACIONES\n",
        "\n",
        "## Montar Google Drive para guardar/cargar archivos fácilmente.\n",
        "Esto es útil para guardar el modelo entrenado y luego descargarlo a tu máquina local.\n"
      ],
      "metadata": {
        "id": "2hkVFCfpKr_M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiZL1KznKdpb",
        "outputId": "cc1cba95-b04a-4491-e1f5-952f151a6460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalar librerías necesarias.\n",
        "'tensorflow-hub' para cargar el modelo YAMNet.\n",
        "'librosa' para el procesamiento de audio (carga, resampleo, aumento de datos).\n",
        "'soundfile' es útil para manejar archivos de audio.\n",
        "'zipfile' se usará para descomprimir el dataset RAVDESS.\n",
        "'scikit-learn' para dividir los datos y escalar características.\n"
      ],
      "metadata": {
        "id": "freGPUBJLm6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Instalando librerías...\")\n",
        "!pip install tensorflow-hub librosa soundfile numpy pandas scikit-learn tensorflow\n",
        "print(\"Librerías instaladas.\")\n",
        "\n",
        "# Importar todas las librerías que usaremos a lo largo del cuaderno.\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout # Dropout es crucial para prevenir el sobreajuste\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import zipfile\n",
        "\n",
        "print(\"Librerías importadas.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrI0xYclLt-H",
        "outputId": "bdbb3c07-19aa-4a7c-f6a0-6386a1f9dc17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instalando librerías...\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (5.29.5)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (2.18.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Librerías instaladas.\n",
            "Librerías importadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 2: Descarga y Organización del Dataset RAVDESS\n",
        "Aquí descargaremos el dataset RAVDESS, un conjunto de datos de emociones en el habla. Luego, lo organizaremos y mapearemos las emociones de RAVDESS a nuestras dos categorías de \"predisposición\" (1) o \"no predisposición\" (0) de manera heurística. Recuerda que este mapeo es una interpretación y puedes ajustarlo si lo consideras necesario."
      ],
      "metadata": {
        "id": "TC0D8_R-MhdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECCIÓN 2: DESCARGA Y ORGANIZACIÓN DEL DATASET RAVDESS\n",
        "\n",
        "URL para descargar RAVDESS. Es posible que tengas que descargarlo manualmente\n",
        "desde el sitio web oficial (buscar \"RAVDESS dataset\") y luego subirlo a Colab o a tu Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "nRpKoW31MpT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si ya lo tienes en tu Google Drive, ajusta la ruta 'RAVDESS_ZIP_PATH' a tu ubicación específica.\n",
        "RAVDESS_ZIP_PATH = '/content/drive/MyDrive/Audio_Speech_Actors_01-24.zip' # RUTA DE EJEMPLO EN DRIVE\n",
        "# Si lo subiste directamente a la sesión de Colab, podría ser 'RAVDESS_Speech_Song_Actors_01-24.zip'\n",
        "\n",
        "# Directorio donde se extraerán los archivos del dataset\n",
        "DATASET_DIR = '/content/ravdess_extracted'\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Descomprimiendo {RAVDESS_ZIP_PATH} en {DATASET_DIR}...\")\n",
        "try:\n",
        "    with zipfile.ZipFile(RAVDESS_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(DATASET_DIR)\n",
        "    print(\"Dataset RAVDESS descomprimido exitosamente.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: El archivo ZIP de RAVDESS no se encontró en '{RAVDESS_ZIP_PATH}'.\")\n",
        "    print(\"Por favor, asegúrate de que el archivo ZIP esté en la ruta especificada (por ejemplo, en tu Google Drive).\")\n",
        "    raise FileNotFoundError(\"RAVDESS ZIP file not found. Please upload or adjust path.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR al descomprimir el dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "# Estructura de nombres de archivo de RAVDESS: Modality-VocalChannel-Emotion-Intensity-Statement-Repetition-Actor.file_extension\n",
        "# El tercer número es el ID de la emoción:\n",
        "# 01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised.\n",
        "\n",
        "# Mapeo de IDs de emoción de RAVDESS a nombres de emoción legibles.\n",
        "emotion_id_to_name = {\n",
        "    '01': 'neutral', '02': 'calm', '03': 'happy', '04': 'sad',\n",
        "    '05': 'angry', '06': 'fearful', '07': 'disgust', '08': 'surprised'\n",
        "}\n",
        "\n",
        "# --- Mapeo HEURÍSTICO de emociones a \"Predispuesto\" (1) o \"No Predispuesto\" (0) ---\n",
        "# Este es el paso crucial donde interpretamos las emociones existentes para tu tarea.\n",
        "# Ajusta este mapeo según tu comprensión de lo que indica \"predisposición\".\n",
        "predisposition_mapping = {\n",
        "    'neutral': 1,      # Un tono neutral puede indicar apertura o predisposición.\n",
        "    'calm': 1,         # La calma a menudo se asocia con disposición positiva.\n",
        "    'happy': 1,        # Felicidad es un fuerte indicador de predisposición.\n",
        "    'surprised': 1,    # Sorpresa (positiva) puede indicar interés o predisposición.\n",
        "    'sad': 0,          # Tristeza suele indicar falta de predisposición.\n",
        "    'angry': 0,        # El enojo claramente indica no predisposición.\n",
        "    'fearful': 0,      # El miedo indica no predisposición.\n",
        "    'disgust': 0       # El disgusto indica no predisposición.\n",
        "}\n",
        "\n",
        "all_audio_files = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Procesando archivos de audio y extrayendo etiquetas de predisposición...\")\n",
        "# Recorrer las carpetas de actores (Actor_01, Actor_02, etc.)\n",
        "for actor_folder in os.listdir(DATASET_DIR):\n",
        "    actor_path = os.path.join(DATASET_DIR, actor_folder)\n",
        "    if os.path.isdir(actor_path): # Asegurarse de que es un directorio\n",
        "        for audio_file in os.listdir(actor_path):\n",
        "            if audio_file.endswith('.wav'): # Solo procesar archivos de audio WAV\n",
        "                file_path = os.path.join(actor_path, audio_file)\n",
        "\n",
        "                # Extraer el ID de la emoción del nombre del archivo (ej: 03-01-03-...)\n",
        "                emotion_id = audio_file.split('-')[2]\n",
        "\n",
        "                # Obtener el nombre de la emoción y luego la etiqueta de predisposición\n",
        "                emotion_name = emotion_id_to_name.get(emotion_id)\n",
        "                if emotion_name: # Si la emoción es reconocida\n",
        "                    predisposition_label = predisposition_mapping.get(emotion_name)\n",
        "                    if predisposition_label is not None: # Si tiene un mapeo a predisposición\n",
        "                        all_audio_files.append(file_path)\n",
        "                        all_labels.append(predisposition_label)\n",
        "\n",
        "# Crear un DataFrame de Pandas para organizar y gestionar los datos de audio y sus etiquetas.\n",
        "df_audio = pd.DataFrame({'file_path': all_audio_files, 'label': all_labels})\n",
        "print(f\"Total de archivos de audio procesados: {len(df_audio)}\")\n",
        "print(\"Distribución de etiquetas de predisposición:\")\n",
        "print(df_audio['label'].value_counts()) # Muestra cuántos audios hay de cada clase (0 o 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlQV78zqL9ys",
        "outputId": "6c8cedae-ad94-48ff-ff0b-4c6d87258997"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimiendo /content/drive/MyDrive/Audio_Speech_Actors_01-24.zip en /content/ravdess_extracted...\n",
            "Dataset RAVDESS descomprimido exitosamente.\n",
            "Procesando archivos de audio y extrayendo etiquetas de predisposición...\n",
            "Total de archivos de audio procesados: 1440\n",
            "Distribución de etiquetas de predisposición:\n",
            "label\n",
            "0    768\n",
            "1    672\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 3: Extracción de Embeddings con YAMNet y Data Augmentation\n",
        "Esta es la parte central del transfer learning. Cargamos YAMNet (un modelo pre-entrenado por Google) y lo usamos para extraer representaciones numéricas de alto nivel (llamadas embeddings) de cada archivo de audio. También implementamos data augmentation (aumento de datos) para crear variaciones de nuestros audios, lo que ayuda al modelo a generalizar mejor y reduce el sobreajuste."
      ],
      "metadata": {
        "id": "aPOwrOmpR2kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECCIÓN 3: EXTRACCIÓN DE EMBEDDINGS CON YAMNET Y DATA AUGMENTATION\n",
        "\n",
        "Cargar el modelo YAMNet de TensorFlow Hub.\n",
        "YAMNet es un modelo pre-entrenado para clasificación de eventos sonoros.\n",
        "Lo utilizaremos como un potente extractor de características de audio."
      ],
      "metadata": {
        "id": "RjygBabqSKCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cargando modelo YAMNet...\")\n",
        "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
        "yamnet_model = hub.load(yamnet_model_handle)\n",
        "print(\"Modelo YAMNet cargado.\")\n",
        "\n",
        "# Configuración para el preprocesamiento de audio, DEBE COINCIDIR con YAMNet y con tu app final.\n",
        "SAMPLE_RATE_YAMNET = 16000 # YAMNet fue entrenado con audio a 16kHz\n",
        "DURATION_SECONDS_YAMNET = 3 # Duración del clip de audio a procesar para el embedding\n",
        "                            # Cada segmento de audio se padeará/truncará a esta duración.\n",
        "\n",
        "def preprocess_audio_for_yamnet_with_augmentation(file_path, sr=SAMPLE_RATE_YAMNET, duration=DURATION_SECONDS_YAMNET, augment=False):\n",
        "    \"\"\"\n",
        "    Carga un archivo de audio, lo resamplea, lo pad/trunca, lo normaliza y, opcionalmente,\n",
        "    aplica técnicas de data augmentation. YAMNet espera la forma de onda de audio (raw waveform).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, current_sr = librosa.load(file_path, sr=sr, mono=True) # Cargar y resamplear\n",
        "\n",
        "        # Normalizar a -1 a 1 para un mejor procesamiento.\n",
        "        y = y / np.max(np.abs(y)) if np.max(np.abs(y)) > 0 else y\n",
        "\n",
        "        # Padear o truncar el audio para asegurar una duración fija.\n",
        "        target_length = sr * duration\n",
        "        if len(y) < target_length:\n",
        "            y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
        "        elif len(y) > target_length:\n",
        "            y = y[:target_length]\n",
        "\n",
        "        # --- APLICAR DATA AUGMENTATION (solo si 'augment' es True) ---\n",
        "        if augment:\n",
        "            # 1. Añadir Ruido Blanco Aleatorio\n",
        "            # Esto ayuda al modelo a ser más robusto a las variaciones de ruido ambiental.\n",
        "            noise_amp = 0.005 * np.random.uniform() * np.amax(y)\n",
        "            y = y + noise_amp * np.random.normal(size=y.shape[0])\n",
        "\n",
        "            # 2. Cambiar Volumen Ligeramente\n",
        "            # Simula diferentes volúmenes de grabación.\n",
        "            y = y * np.random.uniform(low=0.8, high=1.2) # Multiplica por un factor entre 0.8 y 1.2\n",
        "\n",
        "            # 3. Time Stretching (cambio de velocidad sin cambiar el tono)\n",
        "            # Esto hace que el modelo sea robusto a variaciones en la velocidad del habla.\n",
        "            # Asegúrate de que el audio resultante siga teniendo la longitud deseada.\n",
        "            stretch_rate = np.random.uniform(0.9, 1.1) # Alarga o acorta un 10%\n",
        "            y = librosa.effects.time_stretch(y, rate=stretch_rate)\n",
        "            # Volver a padear/truncar si el time stretch cambió la longitud\n",
        "            if len(y) < target_length:\n",
        "                y = np.pad(y, (0, target_length - len(y)), mode='constant')\n",
        "            elif len(y) > target_length:\n",
        "                y = y[:target_length]\n",
        "\n",
        "            # 4. Pitch Shifting (cambio de tono)\n",
        "            # Simula diferentes tonos de voz.\n",
        "            # n_steps: número de semitonos para cambiar el tono (ej: -2 a 2 semitonos).\n",
        "            y = librosa.effects.pitch_shift(y, sr=sr, n_steps=np.random.uniform(-2, 2))\n",
        "\n",
        "            # Normalizar de nuevo después de la aumentación si los cambios fueron significativos\n",
        "            y = y / np.max(np.abs(y)) if np.max(np.abs(y)) > 0 else y\n",
        "\n",
        "        return y.astype(np.float32) # YAMNet espera float32\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al preprocesar (con/sin aug) {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_yamnet_embeddings(audio_waveform, yamnet_model):\n",
        "    \"\"\"\n",
        "    Extrae embeddings de YAMNet de un waveform de audio.\n",
        "    \"\"\"\n",
        "    # YAMNet devuelve scores (probabilidades), embeddings (características) y un log_mel_spectrogram.\n",
        "    # Nos interesan los 'embeddings' para la clasificación.\n",
        "    scores, embeddings, spectrogram = yamnet_model(audio_waveform)\n",
        "\n",
        "    # YAMNet puede devolver una secuencia de embeddings si el audio es largo.\n",
        "    # Para nuestro clasificador final (una red densa simple), promediamos los embeddings\n",
        "    # a lo largo del tiempo para obtener un único vector de características por clip.\n",
        "    if embeddings.shape[0] > 0:\n",
        "        return np.mean(embeddings.numpy(), axis=0) # Convertir a numpy y promediar\n",
        "    else:\n",
        "        return None # Retorna None si no se pudieron extraer embeddings válidos\n",
        "\n",
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Extrayendo embeddings de YAMNet (originales y aumentados) de los archivos de audio...\")\n",
        "# Iterar sobre cada archivo de audio en el DataFrame\n",
        "for index, row in df_audio.iterrows():\n",
        "    file_path = row['file_path']\n",
        "    label = row['label']\n",
        "\n",
        "    # 1. Procesar el audio original (sin aumento)\n",
        "    waveform_orig = preprocess_audio_for_yamnet_with_augmentation(file_path, augment=False)\n",
        "    if waveform_orig is not None:\n",
        "        embedding_orig = extract_yamnet_embeddings(waveform_orig, yamnet_model)\n",
        "        if embedding_orig is not None:\n",
        "            all_embeddings.append(embedding_orig)\n",
        "            all_labels.append(label)\n",
        "\n",
        "    # 2. Procesar versiones aumentadas del audio\n",
        "    # Podemos crear múltiples variantes aumentadas por cada archivo original.\n",
        "    # Esto multiplica efectivamente el tamaño de nuestro dataset de entrenamiento.\n",
        "    NUM_AUGMENTATIONS_PER_SAMPLE = 2 # Puedes ajustar el número de versiones aumentadas\n",
        "    for _ in range(NUM_AUGMENTATIONS_PER_SAMPLE):\n",
        "        waveform_aug = preprocess_audio_for_yamnet_with_augmentation(file_path, augment=True)\n",
        "        if waveform_aug is not None:\n",
        "            embedding_aug = extract_yamnet_embeddings(waveform_aug, yamnet_model)\n",
        "            if embedding_aug is not None:\n",
        "                all_embeddings.append(embedding_aug)\n",
        "                all_labels.append(label)\n",
        "\n",
        "# Convertir las listas a arrays de NumPy para el entrenamiento del modelo.\n",
        "X_embeddings = np.array(all_embeddings)\n",
        "y_labels = np.array(all_labels)\n",
        "\n",
        "# Guardar los embeddings y las etiquetas (opcional, pero útil si la sesión de Colab se desconecta).\n",
        "OUTPUT_EMBEDDINGS_DIR = '/content/audio_embeddings'\n",
        "os.makedirs(OUTPUT_EMBEDDINGS_DIR, exist_ok=True)\n",
        "np.save(os.path.join(OUTPUT_EMBEDDINGS_DIR, 'X_audio_embeddings.npy'), X_embeddings)\n",
        "np.save(os.path.join(OUTPUT_EMBEDDINGS_DIR, 'y_audio_labels.npy'), y_labels)\n",
        "\n",
        "print(f\"Embeddings extraídos (originales + aumentados). Forma de X: {X_embeddings.shape}, Forma de y: {y_labels.shape}\")\n",
        "print(f\"Los embeddings y etiquetas han sido guardados en {OUTPUT_EMBEDDINGS_DIR}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_aP6WAR4DM",
        "outputId": "c662f35b-2f12-4fe6-f755-34e929a533d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo YAMNet...\n",
            "Modelo YAMNet cargado.\n",
            "Extrayendo embeddings de YAMNet (originales y aumentados) de los archivos de audio...\n",
            "Embeddings extraídos (originales + aumentados). Forma de X: (4320, 1024), Forma de y: (4320,)\n",
            "Los embeddings y etiquetas han sido guardados en /content/audio_embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 4: Entrenamiento del Modelo de Clasificación (Fine-tuning)\n",
        "Aquí construiremos y entrenaremos una pequeña red neuronal densa sobre los embeddings extraídos por YAMNet. Esta red actuará como la \"cabeza\" de nuestro modelo de transfer learning, adaptando las características generales de YAMNet a nuestra tarea específica de \"predisposición\". Se han añadido capas Dropout para mejorar la generalización del modelo."
      ],
      "metadata": {
        "id": "qfSFKc3pT3oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECCIÓN 4: ENTRENAMIENTO DEL MODELO DE CLASIFICACIÓN (FINE-TUNING)\n",
        "\n",
        "Cargar los embeddings y etiquetas (si no se procesaron en la sesión actual).\n",
        "Esto es útil si reinicias la sesión de Colab y quieres cargar los datos ya extraídos.\n"
      ],
      "metadata": {
        "id": "NN8rBkzXUGtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    X = np.load(os.path.join(OUTPUT_EMBEDDINGS_DIR, 'X_audio_embeddings.npy'))\n",
        "    y = np.load(os.path.join(OUTPUT_EMBEDDINGS_DIR, 'y_audio_labels.npy'))\n",
        "    print(\"Embeddings y etiquetas cargados de archivos.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: No se encontraron los archivos de embeddings. Asegúrate de ejecutar la sección anterior.\")\n",
        "    raise\n",
        "\n",
        "print(f\"Dimensiones de X: {X.shape}, Dimensiones de y: {y.shape}\")\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba.\n",
        "# test_size=0.2: el 20% de los datos se usarán para probar el modelo.\n",
        "# random_state=42: asegura que la división sea la misma cada vez para reproducibilidad.\n",
        "# stratify=y: crucial para mantener la misma proporción de clases (predispuesto/no predispuesto)\n",
        "# en los conjuntos de entrenamiento y prueba, especialmente si las clases están desequilibradas.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Datos divididos: X_train={X_train.shape}, X_test={X_test.shape}\")\n",
        "\n",
        "# Escalado de características.\n",
        "# Es importante escalar los datos (normalmente a media 0 y varianza 1) para que las\n",
        "# redes neuronales converjan mejor y más rápido.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train) # Ajusta el escalador y transforma los datos de entrenamiento.\n",
        "X_test_scaled = scaler.transform(X_test)       # Transforma los datos de prueba usando el escalador ajustado.\n",
        "\n",
        "print(\"Características escaladas.\")\n",
        "\n",
        "# --- Diseño del Modelo de Clasificación (Red Neuronal con Capas Dropout) ---\n",
        "# Este modelo tomará los embeddings de YAMNet (1024 dimensiones) como entrada.\n",
        "# Añadir capas Dropout es una técnica efectiva para prevenir el sobreajuste.\n",
        "# Dropout \"apaga\" aleatoriamente un porcentaje de neuronas durante el entrenamiento,\n",
        "# forzando a la red a aprender representaciones más robustas y menos dependientes\n",
        "# de neuronas específicas.\n",
        "model = Sequential([\n",
        "    # Capa de entrada: Dense con 256 neuronas, activación 'relu'.\n",
        "    # input_shape: Debe coincidir con la dimensión de los embeddings de YAMNet (1024).\n",
        "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    Dropout(0.5), # Primera capa Dropout: apaga el 50% de las neuronas aleatoriamente.\n",
        "                  # Un valor alto (0.5) es común para las primeras capas densas.\n",
        "\n",
        "    Dense(128, activation='relu'), # Segunda capa oculta: Dense con 128 neuronas.\n",
        "    Dropout(0.4), # Segunda capa Dropout: apaga el 40% de las neuronas.\n",
        "                  # Puedes usar un valor ligeramente menor que la primera.\n",
        "\n",
        "    # Capa de salida: Dense con 1 neurona y activación 'sigmoid'.\n",
        "    # 'sigmoid' es estándar para clasificación binaria, ya que emite un valor entre 0 y 1\n",
        "    # que puede interpretarse como la probabilidad de la clase positiva (predispuesto).\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compilación del Modelo.\n",
        "# optimizer: Algoritmo de optimización (Adam es una excelente opción por defecto).\n",
        "#            learning_rate (tasa de aprendizaje) es crucial: un valor pequeño (0.0001)\n",
        "#            es ideal para fine-tuning sobre características pre-entrenadas.\n",
        "# loss: Función de pérdida (binary_crossentropy es la apropiada para clasificación binaria).\n",
        "# metrics: Métrica para monitorear durante el entrenamiento y evaluación (accuracy es la precisión).\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"\\nResumen del Modelo:\")\n",
        "model.summary()\n",
        "\n",
        "# Entrenamiento del Modelo.\n",
        "# epochs: Número de veces que el modelo verá todos los datos de entrenamiento.\n",
        "#         Ajusta esto: más épocas si el modelo aún mejora, menos si ya se sobreajusta.\n",
        "# batch_size: Número de ejemplos procesados antes de actualizar los pesos del modelo.\n",
        "# validation_split: Porcentaje de los datos de entrenamiento a usar para validación.\n",
        "#                   Estos datos no se usan para actualizar los pesos, solo para monitorear\n",
        "#                   el rendimiento del modelo en datos no vistos durante cada época.\n",
        "print(\"\\nEntrenando el modelo de audio...\")\n",
        "history = model.fit(X_train_scaled, y_train,\n",
        "                    epochs=100, # Un buen punto de partida, puedes aumentar/disminuir.\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2, # El 20% de los datos de entrenamiento se usará para validación.\n",
        "                    verbose=1) # Muestra el progreso del entrenamiento en la salida.\n",
        "\n",
        "# Evaluación del Modelo.\n",
        "# Evalúa el rendimiento final del modelo en los datos de prueba (que nunca ha visto).\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"\\nPrecisión final del modelo en datos de prueba: {accuracy:.4f}\")\n",
        "\n",
        "# Guardar el Modelo.\n",
        "# Guardar el modelo en formato .h5 para poder cargarlo en tu aplicación Streamlit.\n",
        "# Lo guardamos directamente en tu Google Drive para facilitar la descarga.\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/audio_emotion_model.h5\"\n",
        "# Asegúrate de que la carpeta de destino en Drive existe (Colab la crea si no).\n",
        "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"Modelo de audio guardado exitosamente en: {MODEL_SAVE_PATH}\")\n",
        "print(\"Puedes descargarlo desde tu Google Drive y colocarlo en la carpeta 'model/' de tu proyecto Streamlit.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6S6Xi1bIT8aF",
        "outputId": "4162135c-f81c-4ff6-c9d6-0b742ced3bc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings y etiquetas cargados de archivos.\n",
            "Dimensiones de X: (4320, 1024), Dimensiones de y: (4320,)\n",
            "Datos divididos: X_train=(3456, 1024), X_test=(864, 1024)\n",
            "Características escaladas.\n",
            "\n",
            "Resumen del Modelo:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m262,400\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,400</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m295,425\u001b[0m (1.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">295,425</span> (1.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m295,425\u001b[0m (1.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">295,425</span> (1.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entrenando el modelo de audio...\n",
            "Epoch 1/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.5323 - loss: 0.8438 - val_accuracy: 0.5535 - val_loss: 0.6831\n",
            "Epoch 2/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5378 - loss: 0.7929 - val_accuracy: 0.5867 - val_loss: 0.6871\n",
            "Epoch 3/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5584 - loss: 0.7424 - val_accuracy: 0.5882 - val_loss: 0.6728\n",
            "Epoch 4/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5810 - loss: 0.7121 - val_accuracy: 0.5795 - val_loss: 0.7125\n",
            "Epoch 5/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.5795 - loss: 0.7274 - val_accuracy: 0.6069 - val_loss: 0.6589\n",
            "Epoch 6/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6040 - loss: 0.6739 - val_accuracy: 0.6069 - val_loss: 0.6545\n",
            "Epoch 7/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6009 - loss: 0.6695 - val_accuracy: 0.5968 - val_loss: 0.6700\n",
            "Epoch 8/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6145 - loss: 0.6670 - val_accuracy: 0.6199 - val_loss: 0.6521\n",
            "Epoch 9/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6130 - loss: 0.6702 - val_accuracy: 0.6199 - val_loss: 0.6738\n",
            "Epoch 10/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6190 - loss: 0.6627 - val_accuracy: 0.6243 - val_loss: 0.6461\n",
            "Epoch 11/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6174 - loss: 0.6618 - val_accuracy: 0.6199 - val_loss: 0.6466\n",
            "Epoch 12/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6515 - loss: 0.6365 - val_accuracy: 0.6243 - val_loss: 0.6429\n",
            "Epoch 13/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6320 - loss: 0.6486 - val_accuracy: 0.6272 - val_loss: 0.6555\n",
            "Epoch 14/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6308 - loss: 0.6510 - val_accuracy: 0.6315 - val_loss: 0.6385\n",
            "Epoch 15/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6207 - loss: 0.6459 - val_accuracy: 0.6301 - val_loss: 0.6440\n",
            "Epoch 16/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6533 - loss: 0.6260 - val_accuracy: 0.6315 - val_loss: 0.6402\n",
            "Epoch 17/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6517 - loss: 0.6261 - val_accuracy: 0.6431 - val_loss: 0.6311\n",
            "Epoch 18/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6592 - loss: 0.6164 - val_accuracy: 0.6460 - val_loss: 0.6315\n",
            "Epoch 19/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6663 - loss: 0.6108 - val_accuracy: 0.6387 - val_loss: 0.6314\n",
            "Epoch 20/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6803 - loss: 0.5950 - val_accuracy: 0.6532 - val_loss: 0.6302\n",
            "Epoch 21/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6697 - loss: 0.5962 - val_accuracy: 0.6546 - val_loss: 0.6262\n",
            "Epoch 22/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6732 - loss: 0.5985 - val_accuracy: 0.6460 - val_loss: 0.6258\n",
            "Epoch 23/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6870 - loss: 0.5925 - val_accuracy: 0.6532 - val_loss: 0.6271\n",
            "Epoch 24/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6874 - loss: 0.6006 - val_accuracy: 0.6503 - val_loss: 0.6258\n",
            "Epoch 25/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6914 - loss: 0.5843 - val_accuracy: 0.6561 - val_loss: 0.6252\n",
            "Epoch 26/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6909 - loss: 0.5888 - val_accuracy: 0.6503 - val_loss: 0.6222\n",
            "Epoch 27/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6893 - loss: 0.5820 - val_accuracy: 0.6532 - val_loss: 0.6278\n",
            "Epoch 28/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6966 - loss: 0.5737 - val_accuracy: 0.6517 - val_loss: 0.6270\n",
            "Epoch 29/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6968 - loss: 0.5673 - val_accuracy: 0.6503 - val_loss: 0.6253\n",
            "Epoch 30/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7003 - loss: 0.5677 - val_accuracy: 0.6474 - val_loss: 0.6259\n",
            "Epoch 31/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6991 - loss: 0.5800 - val_accuracy: 0.6445 - val_loss: 0.6247\n",
            "Epoch 32/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6988 - loss: 0.5636 - val_accuracy: 0.6488 - val_loss: 0.6341\n",
            "Epoch 33/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6983 - loss: 0.5713 - val_accuracy: 0.6488 - val_loss: 0.6213\n",
            "Epoch 34/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7097 - loss: 0.5637 - val_accuracy: 0.6387 - val_loss: 0.6266\n",
            "Epoch 35/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7097 - loss: 0.5597 - val_accuracy: 0.6517 - val_loss: 0.6154\n",
            "Epoch 36/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7124 - loss: 0.5485 - val_accuracy: 0.6517 - val_loss: 0.6184\n",
            "Epoch 37/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7159 - loss: 0.5485 - val_accuracy: 0.6561 - val_loss: 0.6179\n",
            "Epoch 38/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7297 - loss: 0.5396 - val_accuracy: 0.6503 - val_loss: 0.6258\n",
            "Epoch 39/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7135 - loss: 0.5607 - val_accuracy: 0.6647 - val_loss: 0.6098\n",
            "Epoch 40/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.5210 - val_accuracy: 0.6647 - val_loss: 0.6113\n",
            "Epoch 41/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7197 - loss: 0.5512 - val_accuracy: 0.6561 - val_loss: 0.6275\n",
            "Epoch 42/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7334 - loss: 0.5185 - val_accuracy: 0.6546 - val_loss: 0.6189\n",
            "Epoch 43/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7355 - loss: 0.5309 - val_accuracy: 0.6821 - val_loss: 0.6156\n",
            "Epoch 44/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7345 - loss: 0.5242 - val_accuracy: 0.6561 - val_loss: 0.6153\n",
            "Epoch 45/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7394 - loss: 0.5227 - val_accuracy: 0.6561 - val_loss: 0.6127\n",
            "Epoch 46/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7418 - loss: 0.5172 - val_accuracy: 0.6633 - val_loss: 0.6169\n",
            "Epoch 47/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7410 - loss: 0.5149 - val_accuracy: 0.6633 - val_loss: 0.6161\n",
            "Epoch 48/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7568 - loss: 0.4962 - val_accuracy: 0.6633 - val_loss: 0.6107\n",
            "Epoch 49/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7659 - loss: 0.4918 - val_accuracy: 0.6763 - val_loss: 0.6186\n",
            "Epoch 50/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7647 - loss: 0.4864 - val_accuracy: 0.6763 - val_loss: 0.6043\n",
            "Epoch 51/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4917 - val_accuracy: 0.6720 - val_loss: 0.6156\n",
            "Epoch 52/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7538 - loss: 0.5032 - val_accuracy: 0.6604 - val_loss: 0.6097\n",
            "Epoch 53/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7515 - loss: 0.4996 - val_accuracy: 0.6749 - val_loss: 0.6195\n",
            "Epoch 54/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.4933 - val_accuracy: 0.6676 - val_loss: 0.6155\n",
            "Epoch 55/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7850 - loss: 0.4660 - val_accuracy: 0.6705 - val_loss: 0.6096\n",
            "Epoch 56/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7633 - loss: 0.5079 - val_accuracy: 0.6676 - val_loss: 0.6121\n",
            "Epoch 57/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7867 - loss: 0.4717 - val_accuracy: 0.6647 - val_loss: 0.6109\n",
            "Epoch 58/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7768 - loss: 0.4555 - val_accuracy: 0.6633 - val_loss: 0.6209\n",
            "Epoch 59/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7799 - loss: 0.4729 - val_accuracy: 0.6633 - val_loss: 0.6112\n",
            "Epoch 60/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7921 - loss: 0.4530 - val_accuracy: 0.6647 - val_loss: 0.6176\n",
            "Epoch 61/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7859 - loss: 0.4566 - val_accuracy: 0.6835 - val_loss: 0.6045\n",
            "Epoch 62/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7859 - loss: 0.4527 - val_accuracy: 0.6777 - val_loss: 0.6069\n",
            "Epoch 63/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8016 - loss: 0.4510 - val_accuracy: 0.6705 - val_loss: 0.6130\n",
            "Epoch 64/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7923 - loss: 0.4415 - val_accuracy: 0.6749 - val_loss: 0.6245\n",
            "Epoch 65/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7951 - loss: 0.4340 - val_accuracy: 0.6618 - val_loss: 0.6154\n",
            "Epoch 66/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7753 - loss: 0.4408 - val_accuracy: 0.6763 - val_loss: 0.6184\n",
            "Epoch 67/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7809 - loss: 0.4581 - val_accuracy: 0.6821 - val_loss: 0.6252\n",
            "Epoch 68/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7978 - loss: 0.4317 - val_accuracy: 0.6821 - val_loss: 0.6154\n",
            "Epoch 69/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7959 - loss: 0.4327 - val_accuracy: 0.6763 - val_loss: 0.6187\n",
            "Epoch 70/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8138 - loss: 0.4177 - val_accuracy: 0.6705 - val_loss: 0.6190\n",
            "Epoch 71/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7963 - loss: 0.4350 - val_accuracy: 0.6720 - val_loss: 0.6183\n",
            "Epoch 72/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.4059 - val_accuracy: 0.6720 - val_loss: 0.6245\n",
            "Epoch 73/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8181 - loss: 0.4095 - val_accuracy: 0.6821 - val_loss: 0.6133\n",
            "Epoch 74/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8119 - loss: 0.4131 - val_accuracy: 0.6777 - val_loss: 0.6176\n",
            "Epoch 75/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8146 - loss: 0.4074 - val_accuracy: 0.6821 - val_loss: 0.6211\n",
            "Epoch 76/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8177 - loss: 0.4208 - val_accuracy: 0.6662 - val_loss: 0.6279\n",
            "Epoch 77/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8061 - loss: 0.3955 - val_accuracy: 0.6835 - val_loss: 0.6221\n",
            "Epoch 78/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8241 - loss: 0.3905 - val_accuracy: 0.6618 - val_loss: 0.6200\n",
            "Epoch 79/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8167 - loss: 0.4021 - val_accuracy: 0.6749 - val_loss: 0.6231\n",
            "Epoch 80/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8288 - loss: 0.3768 - val_accuracy: 0.6792 - val_loss: 0.6256\n",
            "Epoch 81/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8328 - loss: 0.3825 - val_accuracy: 0.6676 - val_loss: 0.6403\n",
            "Epoch 82/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8286 - loss: 0.3800 - val_accuracy: 0.6821 - val_loss: 0.6339\n",
            "Epoch 83/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8469 - loss: 0.3719 - val_accuracy: 0.6720 - val_loss: 0.6358\n",
            "Epoch 84/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8336 - loss: 0.3813 - val_accuracy: 0.6705 - val_loss: 0.6329\n",
            "Epoch 85/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8274 - loss: 0.3692 - val_accuracy: 0.6850 - val_loss: 0.6358\n",
            "Epoch 86/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8351 - loss: 0.3710 - val_accuracy: 0.6662 - val_loss: 0.6271\n",
            "Epoch 87/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8307 - loss: 0.3730 - val_accuracy: 0.6633 - val_loss: 0.6382\n",
            "Epoch 88/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8309 - loss: 0.3771 - val_accuracy: 0.6806 - val_loss: 0.6353\n",
            "Epoch 89/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8481 - loss: 0.3456 - val_accuracy: 0.6720 - val_loss: 0.6389\n",
            "Epoch 90/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8494 - loss: 0.3578 - val_accuracy: 0.6835 - val_loss: 0.6358\n",
            "Epoch 91/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8531 - loss: 0.3459 - val_accuracy: 0.6777 - val_loss: 0.6474\n",
            "Epoch 92/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8500 - loss: 0.3362 - val_accuracy: 0.6749 - val_loss: 0.6454\n",
            "Epoch 93/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8522 - loss: 0.3347 - val_accuracy: 0.6734 - val_loss: 0.6532\n",
            "Epoch 94/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8642 - loss: 0.3291 - val_accuracy: 0.6835 - val_loss: 0.6546\n",
            "Epoch 95/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8574 - loss: 0.3199 - val_accuracy: 0.6908 - val_loss: 0.6592\n",
            "Epoch 96/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8547 - loss: 0.3342 - val_accuracy: 0.6749 - val_loss: 0.6676\n",
            "Epoch 97/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8598 - loss: 0.3400 - val_accuracy: 0.6763 - val_loss: 0.6561\n",
            "Epoch 98/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8556 - loss: 0.3209 - val_accuracy: 0.6720 - val_loss: 0.6698\n",
            "Epoch 99/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8603 - loss: 0.3202 - val_accuracy: 0.6821 - val_loss: 0.6531\n",
            "Epoch 100/100\n",
            "\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8674 - loss: 0.3061 - val_accuracy: 0.6777 - val_loss: 0.6558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Precisión final del modelo en datos de prueba: 0.6609\n",
            "Modelo de audio guardado exitosamente en: /content/drive/MyDrive/audio_emotion_model.h5\n",
            "Puedes descargarlo desde tu Google Drive y colocarlo en la carpeta 'model/' de tu proyecto Streamlit.\n"
          ]
        }
      ]
    }
  ]
}