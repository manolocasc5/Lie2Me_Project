# Lie2Me - Detecci√≥n de Predisposici√≥n (V√≠deo y Audio)
Este proyecto es una aplicaci√≥n de an√°lisis multimodal que combina el procesamiento de v√≠deo y audio para detectar emociones y predecir la predisposici√≥n de un individuo (por ejemplo, en el contexto de la interacci√≥n con clientes). Utiliza modelos de aprendizaje profundo para el an√°lisis facial y de voz, ofreciendo una visi√≥n m√°s completa que el an√°lisis de un solo componente.

## üåü Caracter√≠sticas Principales
- An√°lisis Multimodal: Combina la detecci√≥n de emociones faciales (v√≠deo) y la predisposici√≥n emocional basada en el tono de voz (audio) para una predicci√≥n fusionada.

- Detecci√≥n Facial Robusta: Utiliza el clasificador Haar Cascade de OpenCV para la detecci√≥n de rostros en tiempo real.

- Modelos de Transferencia de Aprendizaje:

    - V√≠deo: Modelo MobileNetV2 fine-tuneado para clasificaci√≥n binaria de predisposici√≥n basada en expresiones faciales.

    - Audio: Modelo de red neuronal densa entrenado sobre embeddings de YAMNet (modelo pre-entrenado de Google) para clasificaci√≥n binaria de predisposici√≥n vocal.

- Extracci√≥n y Procesamiento de Audio: Utiliza FFmpeg para extraer la pista de audio de los v√≠deos subidos y librosa para su preprocesamiento y segmentaci√≥n.

- Interfaz Interactiva con Streamlit: Aplicaci√≥n web amigable para la interacci√≥n con el usuario.

- Soporte de Entrada Flexible:

    - C√°mara en Vivo: Captura de v√≠deo desde la webcam con l√≠mite de tiempo (ej. 10 segundos) y redimensionamiento para un mejor rendimiento.

    - Subida de V√≠deo: An√°lisis de v√≠deos desde archivos locales (.mp4, .avi, .mov).

- Visualizaci√≥n Detallada:

    - Gr√°ficos de evoluci√≥n temporal para las predicciones de v√≠deo, audio y la fusi√≥n de ambos.

    - Representaci√≥n de cajas delimitadoras con etiquetas de predisposici√≥n sobre los rostros detectados en el v√≠deo.

- Exportaci√≥n de Resultados: Opci√≥n para guardar el v√≠deo procesado con las anotaciones faciales.

- Control de Ponderaci√≥n: Ajusta el peso de las predicciones de v√≠deo y audio en la fusi√≥n final.

## üìÅ Estructura del Proyecto
.
‚îú‚îÄ‚îÄ app.py                     # Aplicaci√≥n principal de Streamlit

‚îú‚îÄ‚îÄ video_utils.py             # Funciones auxiliares para procesamiento de v√≠deo y audio

‚îú‚îÄ‚îÄ requirements.txt           # Lista de dependencias de Python

‚îú‚îÄ‚îÄ README.md                  # Este archivo de documentaci√≥n

‚îî‚îÄ‚îÄ model/                     # Carpeta para modelos entrenados y archivos auxiliares

    ‚îú‚îÄ‚îÄ mobilenetv2_emotion_binario_finetune.h5 # Modelo de v√≠deo (predicci√≥n facial)
    
    ‚îú‚îÄ‚îÄ audio_emotion_model.h5 # Modelo de audio (predicci√≥n vocal)
    
    ‚îú‚îÄ‚îÄ audio_scaler.npy       # Escalador para los embeddings de audio
    
    ‚îú‚îÄ‚îÄ haarcascade_frontalface_default.xml # Clasificador de Haar para detecci√≥n facial
    
    ‚îî‚îÄ‚îÄ yamnet/                # Carpeta que contiene el modelo YAMNet de TensorFlow Hub
    
        ‚îú‚îÄ‚îÄ saved_model.pb
        
        ‚îî‚îÄ‚îÄ variables/
        
        ‚îî‚îÄ‚îÄ assets/
        
        ‚îî‚îÄ‚îÄ ... (otros archivos de YAMNet)

## üöÄ C√≥mo Usar
### üõ†Ô∏è Entrenamiento (Preparaci√≥n de Modelos)
Se asume que los modelos mobilenetv2_emotion_binario_finetune.h5 y audio_emotion_model.h5 ya est√°n entrenados y guardados en la carpeta model/. Adem√°s, el escalador audio_scaler.npy debe haber sido generado y guardado en la misma carpeta.

Nota: El modelo YAMNet (yamnet/) debe descargarse de TensorFlow Hub y colocarse en la carpeta model/.

### üèÉ Ejecutar la Aplicaci√≥n
#### 1- Clona el repositorio:

git clone <URL_DE_TU_REPOSITORIO>
cd <nombre_del_proyecto>

#### 2- Crea y activa un entorno virtual (muy recomendado):

python -m venv venv

##### En Linux/macOS:
source venv/bin/activate
##### En Windows:
venv\Scripts\activate

#### 3- Instala las dependencias de Python:

pip install -r requirements.txt

#### 4- Instala FFmpeg:
FFmpeg es una herramienta externa esencial para la extracci√≥n de audio de v√≠deos y para guardar los v√≠deos procesados. No se instala con pip.

- En Linux (Ubuntu/Debian):

sudo apt update
sudo apt install ffmpeg

- En macOS (con Homebrew):

brew install ffmpeg

- En Windows:
Descarga los binarios precompilados desde ffmpeg.org/download.html, descompr√≠melos y a√±ade la ruta a la carpeta bin de FFmpeg a las variables de entorno PATH de tu sistema. (Busca tutoriales espec√≠ficos para "a√±adir FFmpeg al PATH en Windows" si necesitas ayuda).

#### 5- Aseg√∫rate de que los modelos est√°n en la carpeta model/:
Verifica que todos los archivos .h5, .npy, .xml y la carpeta yamnet/ est√©n correctamente ubicados dentro del directorio model/.

#### 6- Ejecuta la aplicaci√≥n Streamlit:

streamlit run app.py

#### 7- En la Interfaz Web (tu navegador):

    - Por defecto, la opci√≥n "Subir v√≠deo" estar√° seleccionada. Puedes subir un archivo de v√≠deo (.mp4, .avi, .mov) para un an√°lisis completo (v√≠deo + audio).

    - Alternativamente, puedes elegir "C√°mara en vivo" en la barra lateral para capturar v√≠deo desde tu webcam. La captura se detendr√° autom√°ticamente despu√©s de un tiempo predefinido para el an√°lisis de v√≠deo.

    - Ajusta los pesos de las predicciones de v√≠deo y audio en la barra lateral para influir en la fusi√≥n final.

    - Visualiza los resultados detallados en gr√°ficos y descarga el v√≠deo procesado con las anotaciones.

## üíª Tecnolog√≠as Utilizadas
- Python 3.9+

- TensorFlow / Keras: Para la construcci√≥n y ejecuci√≥n de modelos de aprendizaje profundo (MobileNetV2, Red Neuronal Densa para audio).

- TensorFlow Hub: Para cargar el modelo YAMNet pre-entrenado.

- OpenCV (opencv-python): Para procesamiento de v√≠deo, detecci√≥n facial (Haar Cascade) y dibujo de anotaciones.

- Streamlit: Para la creaci√≥n de la interfaz de usuario web interactiva.

- FFmpeg: Herramienta externa de l√≠nea de comandos para la extracci√≥n de audio de v√≠deo y el guardado de v√≠deo procesado.

- Librosa: Para el preprocesamiento y an√°lisis de se√±ales de audio.

- SoundFile: Soporte para lectura/escritura de archivos de audio (usado por Librosa).

- NumPy: Computaci√≥n num√©rica eficiente.

- Pandas: Manipulaci√≥n y an√°lisis de datos.

- Matplotlib: Generaci√≥n de gr√°ficos y visualizaciones.

- Scikit-learn: Para el escalado de caracter√≠sticas (StandardScaler).

- Joblib: Para guardar y cargar objetos Python eficientemente (el escalador).

## üì¶ requirements.txt
### Core application framework
streamlit==1.30.0

### Deep Learning Framework
tensorflow==2.15.0
tensorflow-hub==0.16.1

### Video Processing
opencv-python==4.9.0.80

### Audio Processing
librosa==0.10.1
soundfile==0.12.1

### Numerical Computing & Data Handling
numpy==1.26.4
pandas==2.2.0
matplotlib==3.8.3
scikit-learn==1.4.0
joblib==1.3.2

## üìÑ Licencia
MIT License

## ‚úíÔ∏è Autores
Alejandro Cabrera y Manolo Castillo
