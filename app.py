import streamlit as st
import cv2
import numpy as np
import tensorflow as tf
import os
import tempfile
from datetime import datetime
import pandas as pd
import math
import matplotlib.pyplot as plt
import subprocess # Necesario para la comprobaci√≥n de FFmpeg

# Importa el m√≥dulo video_utils
# ESTA ES LA PRIMERA Y √öNICA IMPORTACI√ìN DE video_utils.py EN app.py
import video_utils as vu

# --- Configuraci√≥n de la p√°gina de Streamlit ---
st.set_page_config(layout="wide", page_title="Lie2Me - Detecci√≥n de Predisposici√≥n")

st.title("Lie2Me - Detecci√≥n de Predisposici√≥n (V√≠deo + Audio)")

# Carga de modelos y clasificadores
# Se usa st.cache_resource para cargar los modelos solo una vez
# ESTA FUNCI√ìN load_resources() DEBE ESTAR EN app.py, NO EN video_utils.py
@st.cache_resource
def load_resources():
    model_video = None
    model_audio = None

    try:
        model_video = tf.keras.models.load_model("model/mobilenetv2_emotion_binario_finetune.h5")
        st.success("‚úÖ Modelo de v√≠deo cargado correctamente.")
    except Exception as e:
        st.error(f"‚ùå Error al cargar el modelo de v√≠deo: {e}. Aseg√∫rate de que 'model/mobilenetv2_emotion_binario_finetune.h5' existe y es v√°lido.")
        model_video = None

    try:
        model_audio = tf.keras.models.load_model("model/audio_emotion_model.h5")
        st.success("‚úÖ Modelo de audio cargado correctamente.")
    except Exception as e:
        st.error(f"‚ùå Error al cargar el modelo de audio: {e}. Aseg√∫rate de que 'model/audio_emotion_model.h5' existe y es v√°lido.")
        model_audio = None
    
    # Inicializar el clasificador de Haar usando la funci√≥n de video_utils
    status_haar, success_haar = vu.init_face_cascade_classifier()
    if success_haar:
        st.success(f"‚úÖ {status_haar}")
    else:
        st.error(f"‚ùå {status_haar}")

    # YAMNet y el escalador de audio se inicializan globalmente en video_utils
    # Aqu√≠ solo verificamos si se cargaron correctamente al importar vu
    if vu.YAMNET_MODEL is None or vu.audio_scaler is None:
        st.warning("‚ö†Ô∏è El modelo YAMNet o el escalador de audio no se cargaron correctamente. El an√°lisis de audio puede ser inexacto o no realizarse.")
    else:
        st.success("‚úÖ YAMNet y escalador de audio cargados.")

    return model_video, model_audio

# Cargar los recursos al inicio de la aplicaci√≥n
model_video, model_audio = load_resources()

# Inicializar estados de sesi√≥n si no existen
if 'predicciones_video_list' not in st.session_state:
    st.session_state.predicciones_video_list = []
if 'predicciones_audio_list' not in st.session_state:
    st.session_state.predicciones_audio_list = []
if 'frames_procesados_para_guardar' not in st.session_state:
    st.session_state.frames_procesados_para_guardar = []
if 'fps_video_original' not in st.session_state:
    st.session_state.fps_video_original = 30 # Valor por defecto
if 'predicciones_finales_fusionadas' not in st.session_state:
    st.session_state.predicciones_finales_fusionadas = []
if 'predicciones_video_resampled' not in st.session_state:
    st.session_state.predicciones_video_resampled = []
if 'predicciones_audio_resampled' not in st.session_state:
    st.session_state.predicciones_audio_resampled = []
if 'temp_audio_path' not in st.session_state:
    st.session_state.temp_audio_path = None
if 'model_audio_loaded' not in st.session_state:
    # Esta bandera indica si el an√°lisis de audio es *potencialmente* posible
    st.session_state.model_audio_loaded = (model_audio is not None and vu.YAMNET_MODEL is not None and vu.audio_scaler is not None)


# Contenedor para el frame de v√≠deo o mensaje de procesamiento
stframe = st.empty()

# Opciones de fuente de v√≠deo en la barra lateral
st.sidebar.header("Configuraci√≥n de Entrada")
fuente = st.sidebar.radio("Selecciona la fuente de v√≠deo:", ("C√°mara en vivo", "Subir v√≠deo"), index=1)

# Sliders para ponderar las predicciones
st.sidebar.header("Ponderaci√≥n de Predicciones")
video_weight = st.sidebar.slider("Peso de la Predicci√≥n por V√≠deo", 0.0, 1.0, 0.7, 0.05)
audio_weight = st.sidebar.slider("Peso de la Predicci√≥n por Audio", 0.0, 1.0, 0.3, 0.05)

total_weight = video_weight + audio_weight
if total_weight > 0:
    video_weight /= total_weight
    audio_weight /= total_weight
else:
    # Asegurar valores por defecto si la suma es 0
    video_weight = 0.5
    audio_weight = 0.5

st.sidebar.write(f"Pesos normalizados: V√≠deo={video_weight:.2f}, Audio={audio_weight:.2f}")

# --- L√≥gica principal basada en la fuente de v√≠deo ---
if fuente == "C√°mara en vivo":
    st.warning("‚ö†Ô∏è La funcionalidad de c√°mara en vivo est√° en desarrollo y podr√≠a no funcionar en todos los entornos de navegador/servidor.")
    st.info("üí° Para capturar la c√°mara, aseg√∫rate de que tu navegador tiene permisos de acceso a la c√°mara.")
    st.info("‚ùó Nota: El an√°lisis de audio no est√° disponible para la c√°mara en vivo en esta versi√≥n.")

    run_camera = st.checkbox("‚ñ∂Ô∏è Activar C√°mara", key="camera_activation_checkbox")

    cap = None
    if run_camera:
        if not model_video:
            st.error("‚ùå El modelo de v√≠deo no se carg√≥. No se puede realizar el an√°lisis de v√≠deo en vivo.")
            # Desactivar el checkbox de la c√°mara autom√°ticamente si el modelo no est√° listo
            st.session_state.camera_activation_checkbox = False 
            st.stop() # Detiene la ejecuci√≥n aqu√≠ si el modelo no est√° listo

        try:
            cap = cv2.VideoCapture(0) # Intenta abrir la c√°mara
            if not cap.isOpened():
                st.error("‚ùå No se pudo acceder a la c√°mara. Aseg√∫rate de que no est√© en uso por otra aplicaci√≥n y de que los permisos son correctos.")
                cap = None
        except Exception as e:
            st.error(f"‚ùå Error al inicializar la c√°mara: {e}")
            cap = None

        if cap:
            st.session_state.fps_video_original = cap.get(cv2.CAP_PROP_FPS)
            if st.session_state.fps_video_original <= 0:
                st.session_state.fps_video_original = 30 # Fallback si no se obtiene un FPS v√°lido

            # Reiniciar listas para nueva ejecuci√≥n
            st.session_state.predicciones_video_list = []
            st.session_state.predicciones_audio_list = [] # Siempre vac√≠a para c√°mara en vivo
            st.session_state.frames_procesados_para_guardar = []
            st.session_state.predicciones_finales_fusionadas = []
            st.session_state.predicciones_video_resampled = []
            st.session_state.predicciones_audio_resampled = []
            st.session_state.temp_audio_path = None

            frame_count = 0
            # --- PAR√ÅMETROS PARA LA C√ÅMARA EN VIVO ---
            CAPTURA_SEGUNDOS_CAM = 10 # Define cu√°ntos segundos quieres capturar
            MAX_FRAMES_CAPTURA_CAM = int(st.session_state.fps_video_original * CAPTURA_SEGUNDOS_CAM)
            
            st_camera_status = st.empty()
            st_camera_status.info(f"‚è≥ Capturando y procesando v√≠deo de la c√°mara en vivo por {CAPTURA_SEGUNDOS_CAM} segundos...")

            try:
                # Bucle de captura con l√≠mite de frames
                while run_camera and frame_count < MAX_FRAMES_CAPTURA_CAM:
                    ret, frame = cap.read()
                    if not ret:
                        st.warning("‚ö†Ô∏è No se pudieron leer m√°s frames de la c√°mara.")
                        break
                    
                    # Puedes ajustar display_width a tu preferencia (ej. 640, 480, 320).
                    # Cuanto menor sea, m√°s r√°pido y peque√±o se ver√°.
                    display_width = 640 # Ancho deseado para la visualizaci√≥n y procesamiento
                    
                    # Calcula la altura manteniendo la relaci√≥n de aspecto original
                    # Aseg√∫rate de que frame.shape[1] (ancho original) no sea cero para evitar divisi√≥n por cero
                    if frame.shape[1] == 0:
                        st.warning("El ancho del frame de la c√°mara es cero. Omitiendo redimensionamiento.")
                        resized_frame = frame
                    else:
                        display_height = int(frame.shape[0] * (display_width / frame.shape[1]))
                        # Asegurarse de que las dimensiones resultantes sean v√°lidas (positivas)
                        if display_width <= 0 or display_height <= 0:
                            st.warning("Dimensiones de frame inv√°lidas despu√©s de redimensionar. Omitiendo frame.")
                            continue # Saltar al siguiente frame
                        resized_frame = cv2.resize(frame, (display_width, display_height))
                    
                    
                    prediccion_video, frame_con_deteccion = vu.preprocesar_y_predecir_video(resized_frame, model_video)
                    if prediccion_video is not None:
                        st.session_state.predicciones_video_list.append(prediccion_video)
                        st.session_state.frames_procesados_para_guardar.append(frame_con_deteccion)

                    stframe.image(frame_con_deteccion, channels="BGR")
                    frame_count += 1
                    st_camera_status.text(f"üì∏ Capturando... Frame {frame_count}/{MAX_FRAMES_CAPTURA_CAM}")

            finally:
                if cap:
                    cap.release()
                st_camera_status.empty() # Limpia el mensaje de captura
                st.info("‚úÖ C√°mara detenida. Calculando resultados de v√≠deo...")
                
                if st.session_state.predicciones_video_list:
                    # Para la c√°mara en vivo, las predicciones de audio son una lista vac√≠a
                    (st.session_state.predicciones_finales_fusionadas,
                     st.session_state.predicciones_video_resampled,
                     st.session_state.predicciones_audio_resampled) = vu.fusionar_predicciones(
                        st.session_state.predicciones_video_list,
                        [], # Se pasa una lista vac√≠a para audio
                        video_weight,
                        audio_weight
                    )
                    st.success("‚úÖ An√°lisis completado para v√≠deo en vivo!")
                else:
                    st.warning("‚ö†Ô∏è No se capturaron suficientes frames o no se detectaron caras para el an√°lisis de v√≠deo.")
        else:
            st.info("Por favor, haz clic en '‚ñ∂Ô∏è Activar C√°mara' para iniciar la captura.")


elif fuente == "Subir v√≠deo":
    uploaded_file = st.file_uploader("üìÇ Sube un archivo de v√≠deo", type=["mp4", "avi", "mov"])

    if uploaded_file is not None:
        if not model_video:
            st.error("‚ùå El modelo de v√≠deo no se carg√≥. No se puede realizar el an√°lisis de v√≠deo.")
            st.stop()
        if not st.session_state.model_audio_loaded and audio_weight > 0:
            st.warning("‚ö†Ô∏è El modelo de audio o sus dependencias no se cargaron correctamente. Las predicciones se basar√°n solo en el v√≠deo.")

        # Reiniciar listas para nueva ejecuci√≥n
        st.session_state.predicciones_video_list = []
        st.session_state.predicciones_audio_list = []
        st.session_state.frames_procesados_para_guardar = []
        st.session_state.predicciones_finales_fusionadas = []
        st.session_state.predicciones_video_resampled = []
        st.session_state.predicciones_audio_resampled = []
        st.session_state.temp_audio_path = None

        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as temp_video_file:
            temp_video_file.write(uploaded_file.read())
            temp_video_path = temp_video_file.name

        cap = cv2.VideoCapture(temp_video_path)

        if not cap.isOpened():
            st.error("‚ùå Error al abrir el archivo de v√≠deo. Aseg√∫rate de que es un formato compatible y no est√° corrupto.")
            os.unlink(temp_video_path)
            st.stop()

        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        st.session_state.fps_video_original = cap.get(cv2.CAP_PROP_FPS)
        if st.session_state.fps_video_original <= 0: # Fallback
            st.session_state.fps_video_original = 30

        progress_bar = st.progress(0)
        st_status_text = st.empty()

        # Paso de extracci√≥n de audio
        st_status_text.info("üéµ Extrayendo audio del v√≠deo... (Esto puede tardar unos segundos)")
        st.session_state.temp_audio_path = tempfile.NamedTemporaryFile(delete=False, suffix='.wav').name
        try:
            vu.extraer_audio_ffmpeg(temp_video_path, st.session_state.temp_audio_path)
            st_status_text.success("‚úÖ Audio extra√≠do correctamente.")
        except Exception as e:
            st_status_text.error(f"‚ùå Error al extraer audio: {e}. Aseg√∫rate de que FFmpeg est√° instalado y accesible en tu PATH. El an√°lisis de audio ser√° omitido.")
            if os.path.exists(st.session_state.temp_audio_path):
                os.unlink(st.session_state.temp_audio_path)
            st.session_state.temp_audio_path = None # Indicar que no hay audio temporal
            st.session_state.predicciones_audio_list = [] # Asegurarse de que est√© vac√≠a


        # Paso de procesamiento de v√≠deo
        st_status_text.info("üé• Procesando frames de v√≠deo para detecci√≥n facial y emocional...")
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            prediccion_video, frame_con_deteccion = vu.preprocesar_y_predecir_video(frame, model_video)
            if prediccion_video is not None:
                st.session_state.predicciones_video_list.append(prediccion_video)
                st.session_state.frames_procesados_para_guardar.append(frame_con_deteccion)

            frame_count += 1
            progress = min(int((frame_count / total_frames) * 100), 100)
            progress_bar.progress(progress)
            st_status_text.text(f"Procesando frame {frame_count}/{total_frames} (V√≠deo)...")

        cap.release()
        os.unlink(temp_video_path)
        st_status_text.success("‚úÖ An√°lisis de v√≠deo completado.")

        # Paso de procesamiento de audio (solo si el modelo y el audio temporal est√°n disponibles)
        if st.session_state.model_audio_loaded and st.session_state.temp_audio_path and os.path.exists(st.session_state.temp_audio_path):
            st_status_text.info("üéôÔ∏è Analizando audio del v√≠deo...")
            st.session_state.predicciones_audio_list = vu.preprocesar_y_predecir_audio(st.session_state.temp_audio_path, model_audio, vu.YAMNET_MODEL, vu.audio_scaler)
            if st.session_state.predicciones_audio_list is None or not st.session_state.predicciones_audio_list:
                st.session_state.predicciones_audio_list = [] # Asegurar que es una lista vac√≠a si falla
                st.warning("‚ö†Ô∏è No se pudieron obtener predicciones de audio v√°lidas (audio demasiado corto o problema en el procesamiento).")
            else:
                st.success("‚úÖ An√°lisis de audio completado.")
        else:
            st.info("‚ÑπÔ∏è An√°lisis de audio omitido (modelo de audio no cargado o archivo de audio no disponible/v√°lido).")
            st.session_state.predicciones_audio_list = [] # Asegurar que est√© vac√≠a si se omite


        # Fusi√≥n de resultados
        st_status_text.info("‚ú® Fusionando resultados de v√≠deo y audio...")
        (st.session_state.predicciones_finales_fusionadas,
         st.session_state.predicciones_video_resampled,
         st.session_state.predicciones_audio_resampled) = vu.fusionar_predicciones(
            st.session_state.predicciones_video_list,
            st.session_state.predicciones_audio_list,
            video_weight,
            audio_weight
        )
        progress_bar.empty()
        st_status_text.empty()
        st.success("‚úÖ An√°lisis completado!")

        # Limpiar archivo de audio temporal
        if st.session_state.temp_audio_path and os.path.exists(st.session_state.temp_audio_path):
            os.unlink(st.session_state.temp_audio_path)
            st.session_state.temp_audio_path = None


# --- Mostrar resultados y gr√°ficos si hay predicciones ---
def mostrar_resultados():
    predicciones_finales_fusionadas = st.session_state.predicciones_finales_fusionadas
    predicciones_video_resampled = st.session_state.predicciones_video_resampled
    predicciones_audio_resampled = st.session_state.predicciones_audio_resampled

    if predicciones_finales_fusionadas:
        st.subheader("üìä Resultados del An√°lisis de Predisposici√≥n")

        # Predicci√≥n general fusionada
        prediccion_promedio_fusionada = np.mean(predicciones_finales_fusionadas) if predicciones_finales_fusionadas else 0
        col1, col2 = st.columns([1, 2]) # Ajustar proporciones de columna
        with col1:
            st.markdown("#### Predicci√≥n Global (Fusi√≥n)")
            if prediccion_promedio_fusionada > 0.5:
                st.success(f"**Predisposici√≥n Detectada:** {prediccion_promedio_fusionada:.2f}")
                st.balloons()
            else:
                st.error(f"**No Predisposici√≥n Detectada:** {prediccion_promedio_fusionada:.2f}")

        # Gr√°fico de evoluci√≥n temporal de la predisposici√≥n (Fusi√≥n)
        with col2:
            st.markdown("#### Evoluci√≥n de la Probabilidad de Predisposici√≥n (Fusi√≥n)")
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(predicciones_finales_fusionadas, label='Probabilidad (Fusi√≥n)', color='purple', linewidth=2)
            ax.axhline(0.5, color='r', linestyle='--', label='Umbral (0.5)')
            ax.set_xlabel('Segmento de Tiempo')
            ax.set_ylabel('Probabilidad')
            ax.set_title('Evoluci√≥n Temporal de la Predisposici√≥n (Fusi√≥n)')
            ax.legend()
            ax.grid(True)
            st.pyplot(fig)
        
        st.markdown("---") # Separador visual

        # Secci√≥n para predicciones de V√çDEO
        st.subheader("Componente de An√°lisis por V√≠deo")
        if predicciones_video_resampled:
            prediccion_promedio_video = np.mean(st.session_state.predicciones_video_list) if st.session_state.predicciones_video_list else 0
            st.info(f"Probabilidad de predisposici√≥n promedio por **V√≠deo**: **{prediccion_promedio_video:.2f}**")
            
            fig_video, ax_video = plt.subplots(figsize=(10, 3))
            ax_video.plot(predicciones_video_resampled, label='Probabilidad (V√≠deo)', color='blue')
            ax_video.axhline(0.5, color='r', linestyle='--', label='Umbral (0.5)')
            ax_video.set_xlabel('Segmento de Tiempo')
            ax_video.set_ylabel('Probabilidad')
            ax_video.set_title('Evoluci√≥n Temporal de la Predisposici√≥n (V√≠deo)')
            ax_video.legend()
            ax_video.grid(True)
            st.pyplot(fig_video)
        else:
            st.warning("‚ö†Ô∏è No se obtuvieron predicciones de v√≠deo (posiblemente no se detectaron caras o el v√≠deo es muy corto).")


        # Secci√≥n para predicciones de AUDIO
        st.subheader("Componente de An√°lisis por Audio")
        if st.session_state.model_audio_loaded and predicciones_audio_resampled:
            prediccion_promedio_audio = np.mean(st.session_state.predicciones_audio_list) if st.session_state.predicciones_audio_list else 0
            st.info(f"Probabilidad de predisposici√≥n promedio por **Audio**: **{prediccion_promedio_audio:.2f}**")

            fig_audio, ax_audio = plt.subplots(figsize=(10, 3))
            ax_audio.plot(predicciones_audio_resampled, label='Probabilidad (Audio)', color='green')
            ax_audio.axhline(0.5, color='r', linestyle='--', label='Umbral (0.5)')
            ax_audio.set_xlabel('Segmento de Tiempo')
            ax_audio.set_ylabel('Probabilidad')
            ax_audio.set_title('Evoluci√≥n Temporal de la Predisposici√≥n (Audio)')
            ax_audio.legend()
            ax_audio.grid(True)
            st.pyplot(fig_audio)
        else:
            if not st.session_state.model_audio_loaded:
                st.error("‚ùå El modelo de audio o sus dependencias no se pudieron cargar. No se realizaron predicciones de audio.")
            else:
                st.warning("‚ö†Ô∏è No se obtuvieron predicciones de audio v√°lidas (posiblemente el audio es muy corto o el formato no es compatible).")
        
        st.markdown("---") # Separador visual

        # Gr√°fico Comparativo Final
        if predicciones_video_resampled or predicciones_audio_resampled:
            st.subheader("üìà Comparativa de Predicciones (V√≠deo, Audio y Fusi√≥n)")
            fig_comp, ax_comp = plt.subplots(figsize=(12, 6))
            
            if predicciones_video_resampled:
                ax_comp.plot(predicciones_video_resampled, label='Predicci√≥n V√≠deo (Remuestreada)', color='blue', alpha=0.7)
            if predicciones_audio_resampled:
                ax_comp.plot(predicciones_audio_resampled, label='Predicci√≥n Audio (Remuestreada)', color='green', alpha=0.7)
            
            ax_comp.plot(predicciones_finales_fusionadas, label='Predicci√≥n Fusionada', color='purple', linewidth=2)
            ax_comp.axhline(0.5, color='r', linestyle='--', label='Umbral (0.5)')
            ax_comp.set_xlabel('Segmento de Tiempo (Base de remuestreo)')
            ax_comp.set_ylabel('Probabilidad')
            ax_comp.set_title('Comparativa de Predicciones de Predisposici√≥n')
            ax_comp.legend()
            ax_comp.grid(True)
            st.pyplot(fig_comp)


        # Ofrecer guardar el v√≠deo procesado con detecciones faciales
        if st.session_state.frames_procesados_para_guardar:
            st.markdown("---") # Separador visual
            st.subheader("Guardar V√≠deo Procesado")
            if st.button("‚¨áÔ∏è Guardar y Descargar V√≠deo Procesado"):
                output_video_filename = f"video_procesado_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
                temp_output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name

                try:
                    with st.spinner("‚è≥ Generando v√≠deo procesado... Esto puede tardar unos minutos para v√≠deos largos."):
                        vu.guardar_frames_como_video(st.session_state.frames_procesados_para_guardar, temp_output_path, int(st.session_state.fps_video_original))
                    
                    with open(temp_output_path, "rb") as file:
                        st.download_button(
                            label="Haz clic para descargar el v√≠deo",
                            data=file.read(),
                            file_name=output_video_filename,
                            mime="video/mp4"
                        )
                    st.success(f"‚úÖ V√≠deo guardado exitosamente en: {output_video_filename}")
                    os.unlink(temp_output_path) # Limpiar archivo temporal
                except Exception as e:
                    st.error(f"‚ùå Error al guardar o descargar el v√≠deo procesado: {e}")
                    st.warning("Aseg√∫rate de que FFmpeg est√° instalado correctamente en tu sistema para el guardado de v√≠deo.")

        # Limpiar las listas de predicciones y frames guardados para la siguiente ejecuci√≥n
        # Esto es crucial para que los resultados no persistan si el usuario sube otro v√≠deo
        st.session_state.predicciones_video_list = []
        st.session_state.predicciones_audio_list = []
        st.session_state.frames_procesados_para_guardar = []
        st.session_state.predicciones_finales_fusionadas = []
        st.session_state.predicciones_video_resampled = []
        st.session_state.predicciones_audio_resampled = []
        st.session_state.temp_audio_path = None


# Llama a mostrar_resultados al final del script si hay predicciones para mostrar
if st.session_state.predicciones_finales_fusionadas:
    mostrar_resultados()
else:
    st.info("‚ÑπÔ∏è Esperando la captura o subida de v√≠deo para iniciar el an√°lisis.")
    st.markdown("---")
    st.markdown("### Estado de los Recursos:")
    # Mostrar el estado inicial de la carga de modelos para ayudar al usuario
    # El estado de carga de model_video y model_audio ya se muestra al inicio
    
    # Comprobar estado de Haar Cascade
    status_haar, success_haar = vu.init_face_cascade_classifier() # Re-chequear por si acaso
    if not success_haar:
        st.error(f"‚ùå Clasificador de Haar no listo: {status_haar}. La detecci√≥n facial no funcionar√°.")
    else:
        st.success("‚úÖ Clasificador de Haar listo.")
    
    # Comprobar estado de YAMNet y escalador (sus variables globales est√°n en vu)
    if not vu.YAMNET_MODEL:
        st.error("‚ùå YAMNet no cargado. El an√°lisis de audio puede fallar.")
    else:
        st.success("‚úÖ YAMNet cargado.")

    if not vu.audio_scaler:
        st.error("‚ùå Escalador de audio no cargado. El an√°lisis de audio puede ser impreciso.")
    else:
        st.success("‚úÖ Escalador de audio cargado.")

    # Comprobar FFmpeg (solo una comprobaci√≥n b√°sica, la real ocurre en las funciones)
    try:
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True, text=True)
        st.success("‚úÖ FFmpeg detectado y accesible en tu PATH.")
    except (subprocess.CalledProcessError, FileNotFoundError):
        st.error("‚ùå FFmpeg no detectado o no accesible en tu PATH. La extracci√≥n y guardado de audio/v√≠deo no funcionar√°n.")
    
    st.warning("‚òùÔ∏è Aseg√∫rate de que todos los archivos necesarios (.h5, .npy, .xml y la carpeta 'yamnet') est√°n en la carpeta 'model/' en el mismo directorio.")