import streamlit as st
import cv2
import numpy as np
import tempfile
import time
import video_utils as vu 
import tensorflow as tf
import pandas as pd
import os
import subprocess 

# --- Configuraci√≥n de la p√°gina de Streamlit ---
st.set_page_config(page_title="Lie2Me - Emociones en tiempo real", layout="wide")
st.title("üé≠ Lie2Me - Detecci√≥n de emociones y predisposici√≥n (Video + Audio)")

# --- Inicializar y verificar el clasificador de Haar (sin cambios) ---
face_cascade_status_message, face_cascade_loaded_successfully = vu.init_face_cascade_classifier()

if face_cascade_loaded_successfully:
    st.sidebar.success(face_cascade_status_message)
else:
    st.sidebar.error(face_cascade_status_message)
    st.stop() 

# --- Carga del modelo TensorFlow de VIDEO ---
@st.cache_resource # Cach√© para no cargarlo cada vez
def cargar_modelo_video():
    try:
        model_path = "model/mobilenetv2_emotion_binario_finetune.h5"
        if not os.path.exists(model_path):
            st.error(f"Error: El archivo del modelo de v√≠deo no se encuentra en la ruta: {model_path}")
            st.stop()
        model = tf.keras.models.load_model(model_path)
        return model
    except Exception as e:
        st.error(f"Error al cargar el modelo de v√≠deo de TensorFlow: {e}")
        st.stop()

model_video = cargar_modelo_video()

# --- ¬°NUEVO! Carga del modelo TensorFlow de AUDIO ---
@st.cache_resource # Cach√© para no cargarlo cada vez
def cargar_modelo_audio():
    try:
        # ¬°IMPORTANTE! Reemplaza esto con la ruta a TU modelo de audio real.
        audio_model_path = "model/audio_emotion_model.h5" 
        if not os.path.exists(audio_model_path):
            st.warning(f"Advertencia: El archivo del modelo de audio no se encuentra en la ruta: {audio_model_path}. La predicci√≥n de audio no estar√° disponible.")
            return None # Retorna None si no se encuentra
        model = tf.keras.models.load_model(audio_model_path)
        return model
    except Exception as e:
        st.warning(f"Error al cargar el modelo de audio de TensorFlow: {e}. La predicci√≥n de audio no estar√° disponible. Aseg√∫rate de que el modelo es compatible.")
        return None

model_audio = cargar_modelo_audio()
if model_audio:
    st.sidebar.success("Modelo de audio cargado correctamente.")
else:
    st.sidebar.info("Modelo de audio no disponible. Las predicciones se realizar√°n solo por v√≠deo.")


# --- Variables para almacenar resultados globales ---
predicciones_video_totales = [] # Lista de predicciones del v√≠deo por cada detecci√≥n facial
prediccion_audio_final = None   # Predicci√≥n √∫nica del audio de todo el v√≠deo
predicciones_finales_fusionadas = [] # Predicci√≥n combinada (una para v√≠deos subidos)
frames_procesados_para_guardar = []

# --- Sidebar: Selecci√≥n de la fuente de v√≠deo ---
fuente = st.sidebar.radio("Selecciona la fuente de v√≠deo:", ("C√°mara en vivo", "Subir v√≠deo"))

# --- ¬°NUEVO! Control de ponderaci√≥n en la sidebar ---
st.sidebar.markdown("---")
st.sidebar.header("Ponderaci√≥n de Predicciones")
# Solo muestra los sliders si el modelo de audio est√° cargado
if model_audio:
    peso_video = st.sidebar.slider("Peso de la Predicci√≥n por V√≠deo", 0.0, 1.0, 0.7, 0.05)
    peso_audio = st.sidebar.slider("Peso de la Predicci√≥n por Audio", 0.0, 1.0, 0.3, 0.05)
    
    # Normalizar pesos para que sumen 1 (siempre)
    total_pesos = peso_video + peso_audio
    if total_pesos > 0:
        peso_video_normalizado = peso_video / total_pesos
        peso_audio_normalizado = peso_audio / total_pesos
    else: 
        # Si ambos sliders est√°n a 0, damos un valor por defecto para evitar divisi√≥n por cero
        peso_video_normalizado = 0.5
        peso_audio_normalizado = 0.5
    
    st.sidebar.info(f"Pesos normalizados: V√≠deo={peso_video_normalizado:.2f}, Audio={peso_audio_normalizado:.2f}")
else:
    st.sidebar.info("La ponderaci√≥n del audio est√° deshabilitada ya que el modelo de audio no fue cargado.")
    peso_video_normalizado = 1.0 # Si no hay audio, el v√≠deo tiene todo el peso
    peso_audio_normalizado = 0.0

# --- Funci√≥n para procesar un solo frame (predicci√≥n de v√≠deo) ---
# Se le pasa el modelo de v√≠deo ahora
def procesar_frame(frame_input, video_model):
    boxes = vu.detectar_rostros(frame_input)
    preds_en_frame = []

    for box in boxes:
        rostro = vu.extraer_region_rostro(frame_input, box)
        if rostro is not None:
            img_proc = vu.preprocesar_imagen(rostro)
            pred = vu.predecir_emocion(video_model, img_proc) # Usamos el modelo de v√≠deo
            preds_en_frame.append(pred)
            
            texto = f"Predispuesto ({pred:.2f})" if pred > 0.5 else f"No predispuesto ({pred:.2f})"
            color = (0, 255, 0) if pred > 0.5 else (0, 0, 255)

            vu.dibujar_caja_y_texto(frame_input, box, texto, color=color)
        else:
            preds_en_frame.append(None)
    return frame_input, boxes, preds_en_frame

# --- L√≥gica principal basada en la fuente de v√≠deo ---
stframe = st.empty()

if fuente == "C√°mara en vivo":
    st.warning("La predicci√≥n de audio no es compatible con la c√°mara en vivo en esta versi√≥n. Se realizar√° solo la predicci√≥n por v√≠deo.")
    st.write("üé• Capturando desde c√°mara por 5 segundos...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        st.error("Error: No se pudo acceder a la c√°mara. Aseg√∫rate de que est√© conectada y no est√© en uso por otra aplicaci√≥n.")
        st.stop()

    start_time = time.time()
    duration = 5
    
    while time.time() - start_time < duration:
        ret, frame = cap.read()
        if not ret:
            st.warning("No se pudo leer el frame desde la c√°mara. Intentando de nuevo...")
            time.sleep(0.1)
            continue

        processed_frame, current_boxes, current_preds = procesar_frame(frame, model_video)
        predicciones_video_totales.extend([p for p in current_preds if p is not None])
        frames_procesados_para_guardar.append(processed_frame.copy())

        frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
        stframe.image(frame_rgb, channels="RGB")

    cap.release()
    # Para la c√°mara en vivo, la predicci√≥n combinada es simplemente la de v√≠deo
    predicciones_finales_fusionadas = predicciones_video_totales


elif fuente == "Subir video":
    uploaded_file = st.file_uploader("Sube un archivo de v√≠deo", type=["mp4", "avi", "mov"])
    
    if uploaded_file is not None:
        # 1. Guardar el archivo subido en un archivo temporal
        original_video_path = ""
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tfile:
            tfile.write(uploaded_file.read())
            original_video_path = tfile.name

        # 2. Determinar la ruta para el v√≠deo pre-procesado (rotaci√≥n)
        processed_video_path = original_video_path.replace(".mp4", "_processed.mp4")
        if not processed_video_path.endswith(".mp4"):
            processed_video_path = processed_video_path + ".mp4"
            
        st.info("Detectando y corrigiendo la orientaci√≥n del v√≠deo (si es necesario)...")
        
        ffmpeg_command = [
            "ffmpeg",
            "-i", original_video_path,
            "-vf", "transpose=2,transpose=2,transpose=1,transpose=1", 
            "-c:v", "libx264", 
            "-preset", "fast", 
            "-crf", "23", 
            "-c:a", "aac", 
            "-b:a", "128k", 
            "-y", 
            processed_video_path
        ]
        
        try:
            subprocess.run(ffmpeg_command, capture_output=True, text=True, check=True)
            st.success("V√≠deo pre-procesado para corregir orientaci√≥n.")
        except subprocess.CalledProcessError as e:
            st.error(f"Error al pre-procesar el v√≠deo con FFmpeg: {e.stderr}")
            st.warning("Intentando procesar el v√≠deo original sin correcci√≥n de rotaci√≥n. Podr√≠a aparecer invertido.")
            processed_video_path = original_video_path 
        except FileNotFoundError:
            st.error("Error: FFmpeg no encontrado. Aseg√∫rate de que FFmpeg est√° instalado y en tu PATH.")
            st.warning("Intentando procesar el v√≠deo original sin correcci√≥n de rotaci√≥n. Podr√≠a aparecer invertido.")
            processed_video_path = original_video_path 
        except Exception as e:
            st.error(f"Error inesperado durante el pre-procesamiento: {e}")
            st.warning("Intentando procesar el v√≠deo original sin correcci√≥n de rotaci√≥n. Podr√≠a aparecer invertido.")
            processed_video_path = original_video_path 

        # --- ¬°NUEVO! Procesamiento de Audio ---
        audio_output_path = original_video_path.replace(".mp4", ".wav")
        if model_audio: # Solo si se carg√≥ el modelo de audio
            st.info("Extrayendo y procesando audio...")
            if vu.extract_audio_from_video(original_video_path, audio_output_path):
                audio_features = vu.preprocess_audio(audio_output_path)
                if audio_features is not None:
                    prediccion_audio_final = vu.predict_audio_emotion(model_audio, audio_features)
                    st.success(f"Predicci√≥n de Audio: {prediccion_audio_final:.2f}")
                else:
                    st.warning("No se pudieron extraer caracter√≠sticas de audio v√°lidas.")
            else:
                st.warning("No se pudo extraer el audio del v√≠deo. La predicci√≥n de audio no estar√° disponible.")
        else:
            st.info("El modelo de audio no est√° cargado, se omitir√° el procesamiento de audio.")

        # 3. Abrir el v√≠deo PROCESADO (o el original si hubo error)
        cap = cv2.VideoCapture(processed_video_path)
        
        if not cap.isOpened():
            st.error(f"Error: No se pudo abrir el archivo de v√≠deo: {os.path.basename(processed_video_path)}. ¬øEs un archivo de v√≠deo v√°lido despu√©s del procesamiento?")
            # Limpiar archivos temporales
            if os.path.exists(original_video_path): os.unlink(original_video_path)
            if os.path.exists(processed_video_path): os.unlink(processed_video_path)
            if os.path.exists(audio_output_path): os.unlink(audio_output_path) # Limpiar audio temporal
            st.stop()

        MAX_FRAME_WIDTH = 800 

        progress_bar = st.progress(0)
        frame_count = 0
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0: 
            st.warning("El v√≠deo parece estar vac√≠o o corrupto (0 frames) despu√©s del procesamiento.")
            cap.release()
            if os.path.exists(original_video_path): os.unlink(original_video_path)
            if os.path.exists(processed_video_path): os.unlink(processed_video_path)
            if os.path.exists(audio_output_path): os.unlink(audio_output_path)
            st.stop()

        st.write(f"Procesando v√≠deo: {total_frames} frames...")

        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            current_height, current_width, _ = frame.shape
            if current_width > MAX_FRAME_WIDTH:
                new_width = MAX_FRAME_WIDTH
                new_height = int(current_height * (MAX_FRAME_WIDTH / current_width))
                frame = cv2.resize(frame, (new_width, new_height), interpolation=cv2.INTER_AREA)

            processed_frame, current_boxes, current_preds = procesar_frame(frame, model_video)
            predicciones_video_totales.extend([p for p in current_preds if p is not None])
            frames_procesados_para_guardar.append(processed_frame.copy())

            frame_rgb = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
            stframe.image(frame_rgb, channels="RGB")

            frame_count += 1
            progress_bar.progress(min(100, int((frame_count / total_frames) * 100)))

        cap.release()
        # Eliminar archivos temporales al finalizar
        if os.path.exists(original_video_path): os.unlink(original_video_path)
        if os.path.exists(processed_video_path): os.unlink(processed_video_path)
        if os.path.exists(audio_output_path): os.unlink(audio_output_path) # Limpiar audio temporal

        # --- ¬°NUEVO! Fusi√≥n de Predicciones ---
        if predicciones_video_totales and prediccion_audio_final is not None:
            st.markdown("---")
            st.header("Combinando Predicciones de V√≠deo y Audio")
            
            # Calculamos el promedio de todas las predicciones de v√≠deo
            promedio_pred_video = np.mean(predicciones_video_totales)
            st.write(f"Promedio de Predicciones de V√≠deo: **{promedio_pred_video:.2f}**")
            st.write(f"Predicci√≥n de Audio: **{prediccion_audio_final:.2f}**")

            # Aplicamos la fusi√≥n ponderada
            prediccion_fusionada = (promedio_pred_video * peso_video_normalizado) + \
                                   (prediccion_audio_final * peso_audio_normalizado)
            
            st.success(f"**Predicci√≥n Final Combinada (V√≠deo x{peso_video_normalizado:.2f} + Audio x{peso_audio_normalizado:.2f}): {prediccion_fusionada:.2f}**")
            predicciones_finales_fusionadas.append(prediccion_fusionada) # Guardamos la predicci√≥n final

        elif predicciones_video_totales:
            st.warning("Solo se proces√≥ la predicci√≥n de v√≠deo (no hay audio o el modelo de audio no se carg√≥).")
            # Si solo hay predicciones de v√≠deo, estas ser√°n las "finales"
            predicciones_finales_fusionadas = predicciones_video_totales
        elif prediccion_audio_final is not None:
            st.warning("Solo se proces√≥ la predicci√≥n de audio (no hay v√≠deo o detecci√≥n de rostros).")
            # Si solo hay predicci√≥n de audio, esa ser√° la "final"
            predicciones_finales_fusionadas.append(prediccion_audio_final)

# --- Secci√≥n de resultados y descargas (adaptada para usar las predicciones fusionadas) ---
if predicciones_finales_fusionadas:
    st.markdown("---")
    st.header("üìä Resultados del An√°lisis")

    # Contamos las predicciones por encima y por debajo del umbral de 0.5
    count_pos = sum(1 for p in predicciones_finales_fusionadas if p is not None and p > 0.5)
    count_neg = sum(1 for p in predicciones_finales_fusionadas if p is not None and p <= 0.5)
    
    total_preds_validas = len([p for p in predicciones_finales_fusionadas if p is not None])

    st.write(f"‚úÖ **Detecciones Predispuestas:** {count_pos} de {total_preds_validas}")
    st.write(f"‚ùå **Detecciones No Predispuestas:** {count_neg} de {total_preds_validas}")

    if total_preds_validas > 0:
        # La conclusi√≥n principal se basa en el promedio o el √∫nico resultado fusionado
        if len(predicciones_finales_fusionadas) == 1 and fuente == "Subir video":
            final_pred_value = predicciones_finales_fusionadas[0]
            if final_pred_value > 0.5:
                st.success("‚úÖ **Conclusi√≥n Final (V√≠deo + Audio):** Se detecta una **predisposici√≥n a repetir la experiencia**.")
            else:
                st.error("‚ùå **Conclusi√≥n Final (V√≠deo + Audio):** Se detecta **no predisposici√≥n a repetir la experiencia**.")
        else: # Para c√°mara en vivo o cuando solo hay predicciones de v√≠deo
            if count_pos > count_neg:
                st.success("‚úÖ **Conclusi√≥n:** La mayor√≠a de las detecciones indican una **predisposici√≥n a repetir la experiencia**.")
            elif count_neg > count_pos:
                st.error("‚ùå **Conclusi√≥n:** La mayor√≠a de las detecciones indican **no predisposici√≥n a repetir la experiencia**.")
            else:
                st.info("‚ÑπÔ∏è **Conclusi√≥n:** Las detecciones de predisposici√≥n y no predisposici√≥n est√°n equilibradas.")
    else:
        st.warning("No se pudieron obtener predicciones v√°lidas para la conclusi√≥n.")

    if predicciones_video_totales: # Mostramos la gr√°fica de v√≠deo si hay datos
        fig_video = vu.graficar_predicciones(predicciones_video_totales, titulo="Evoluci√≥n de la Predisposici√≥n por V√≠deo")
        st.pyplot(fig_video)
    
    # Si hay una predicci√≥n fusionada (para v√≠deos subidos), la mostramos de forma destacada
    if len(predicciones_finales_fusionadas) == 1 and fuente == "Subir video":
        st.markdown("---")
        st.subheader("Predicci√≥n Final Combinada")
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="Predicci√≥n Final", value=f"{predicciones_finales_fusionadas[0]:.2f}")
        with col2:
            if predicciones_finales_fusionadas[0] > 0.5:
                st.success("Resultado: Predispuesto")
            else:
                st.error("Resultado: No Predispuesto")

    # CSV de predicciones (ahora incluye las columnas de audio y fusionada si aplican)
    if predicciones_video_totales:
        df_preds_dict = {
            "id_deteccion": list(range(len(predicciones_video_totales))),
            "prediccion_video_raw": predicciones_video_totales,
            "predisposicion_video": ["Predispuesto" if p > 0.5 else "No predispuesto" for p in predicciones_video_totales]
        }
        
        if prediccion_audio_final is not None:
            # Repetimos la predicci√≥n de audio para cada detecci√≥n de v√≠deo para que el DataFrame tenga la misma longitud
            df_preds_dict["prediccion_audio_raw"] = [prediccion_audio_final] * len(predicciones_video_totales) 
            df_preds_dict["predisposicion_audio"] = ["Predispuesto" if prediccion_audio_final > 0.5 else "No predispuesto"] * len(predicciones_video_totales)
            
            # La predicci√≥n fusionada tambi√©n se repite, ya que es un valor √∫nico para todo el v√≠deo
            df_preds_dict["prediccion_fusionada_raw"] = [predicciones_finales_fusionadas[0]] * len(predicciones_video_totales) if predicciones_finales_fusionadas else [0.0] * len(predicciones_video_totales)
            df_preds_dict["predisposicion_fusionada"] = ["Predispuesto" if p > 0.5 else "No predispuesto" for p in df_preds_dict["prediccion_fusionada_raw"]]

        df_preds = pd.DataFrame(df_preds_dict)
        csv = df_preds.to_csv(index=False).encode("utf-8")
        st.download_button(
            label="‚¨áÔ∏è Descargar predicciones CSV",
            data=csv,
            file_name="predicciones_lie2me.csv",
            mime="text/csv"
        )

    # ... (El resto del c√≥digo para guardar el v√≠deo anotado permanece igual) ...

else:
    st.info("Esperando la captura o subida de v√≠deo para iniciar el an√°lisis. No se han procesado frames a√∫n.")